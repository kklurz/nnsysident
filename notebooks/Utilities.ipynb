{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datajoint as dj\n",
    "dj.config['database.host'] = os.environ['DJ_HOST']\n",
    "dj.config['database.user'] = os.environ['DJ_USER']\n",
    "dj.config['database.password'] = os.environ['DJ_PASS']\n",
    "dj.config['enable_python_native_blobs'] = True\n",
    "\n",
    "name = \"simdata\"\n",
    "dj.config['schema_name'] = f\"konstantin_nnsysident_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nnfabrik\n",
    "from nnfabrik.main import *\n",
    "from nnfabrik import builder\n",
    "from nnfabrik.utility.hypersearch import Bayesian\n",
    "\n",
    "from nnsysident.tables.experiments import *\n",
    "from nnsysident.tables.bayesian import *\n",
    "from nnsysident.datasets.mouse_loaders import static_shared_loaders\n",
    "from nnsysident.datasets.mouse_loaders import static_loaders\n",
    "from nnsysident.datasets.mouse_loaders import static_loader\n",
    "\n",
    "def find_number(text, c):\n",
    "    return re.findall(r'%s(\\d+)' % c, text)\n",
    "\n",
    "def get_transfer(old_experiment_name):\n",
    "    # prepare the Transfer table in a way that all the info about the transferred model is in the DataFrame. Just pd.merge (on transfer_fn and transfer_hash)\n",
    "    # it then with the model that the transferred model was used for. \n",
    "    transfer = pd.DataFrame(Transfer.fetch())\n",
    "    transfer = pd.concat([transfer, transfer['transfer_config'].apply(pd.Series)], axis = 1).drop('transfer_config', axis = 1)\n",
    "\n",
    "    tm = pd.DataFrame((TrainedModel * Dataset * Seed * Experiments.Restrictions & 'experiment_name = \"{}\"'.format(old_experiment_name)).fetch()).rename(\n",
    "        columns = {'model_hash': 't_model_hash', 'trainer_hash': 't_trainer_hash', 'dataset_hash': 't_dataset_hash'})\n",
    "    tm = tm.sort_values('score', ascending=False).drop_duplicates(['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "\n",
    "    transfer = pd.merge(transfer, tm, how='inner', on=['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "    transfer = pd.concat([transfer, transfer['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "    transfer.columns = ['t_' + col if col[:2] != 't_' and col[:8] != 'transfer'  else col for col in transfer.columns]\n",
    "    transfer = transfer.sort_values(['t_neuron_n', 't_image_n', 't_neuron_base_seed', 't_image_base_seed'])\n",
    "    return transfer\n",
    "\n",
    "def get_transfer_entries(old_experiment_name, overall_best):\n",
    "    tm = pd.DataFrame((TrainedModel * Dataset * Seed * Experiments.Restrictions & 'experiment_name=\"{}\"'.format(old_experiment_name)).fetch())\n",
    "    tm = pd.concat([tm, tm['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "\n",
    "    model_fn = np.unique(tm['model_fn'])\n",
    "    assert len(model_fn) == 1 ,\"Must have exactly 1 model function in experiment\"\n",
    "    model_fn = model_fn[0] \n",
    "\n",
    "    # Filter out best model(s) \n",
    "    if overall_best is True:\n",
    "        tm = tm.loc[(tm['neuron_n'] == tm['neuron_n'].max()) & (tm['image_n'] == tm['image_n'].max())]\n",
    "    tm = tm.sort_values('score', ascending=False).drop_duplicates(['neuron_n', 'image_n', 'neuron_base_seed', 'image_base_seed']).sort_values(['neuron_n', 'image_n'])\n",
    "\n",
    "    # make entries for Trasfer table\n",
    "    entries = [dict(transfer_fn='nnsysident.models.transfer_functions.core_transfer', \n",
    "                     transfer_config = dict(t_model_hash=row.model_hash, t_dataset_hash=row.dataset_hash, t_trainer_hash=row.trainer_hash),\n",
    "                     transfer_comment=model_fn.split('.')[-1] + ', neuron_n={}, neuron_base_seed={}, image_n={}, image_base_seed={}'.format(row.neuron_n, \n",
    "                                                                                                                                    row.neuron_base_seed, \n",
    "                                                                                                                                    row.image_n, \n",
    "                                                                                                                                    row.image_base_seed),\n",
    "                     transfer_fabrikant='kklurz') for loc, row in tm.iterrows()]\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (direkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dataset entries for different neuron and image seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_base_seeds = [1,2,3]\n",
    "image_base_seeds = [1,2,3]\n",
    "\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "paths = ['data/static0-0-3-preproc0.zip']\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[(dataset['neuron_base_seed'] == 1) & (dataset['image_base_seed'] == 1) & (dataset['paths'].isin([paths]))]\n",
    "dataset = dataset.loc[dataset['exclude_neuron_n'].isnull()]\n",
    "\n",
    "for loc, row in dataset.iterrows():\n",
    "    for neuron_base_seed in neuron_base_seeds:\n",
    "        for image_base_seed in image_base_seeds:\n",
    "            dataset_config = row['dataset_config']\n",
    "            dataset_config.update(neuron_base_seed=neuron_base_seed, image_base_seed=image_base_seed)\n",
    "            Dataset().add_entry(dataset_fn=row['dataset_fn'], \n",
    "                                dataset_config=dataset_config, \n",
    "                                dataset_fabrikant=row['dataset_fabrikant'], \n",
    "                                dataset_comment=row['dataset_comment'], skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static0-0-3-preproc0.zip']\n",
    "\n",
    "experiment_name = 'SIM, Direct, {}, 0-0-3'.format(model_name)\n",
    "experiment_comment = 'Directly trained on simulated data with {} and static_loaders of static0-0-3-preproc0.zip. Varying number of neurons and images.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[dataset['paths'].isin([paths])]\n",
    "dataset = dataset.loc[dataset['exclude_neuron_n'].isnull()]\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [int(find_number(row.model_comment, arg + '=')[0]) for loc, row in model.iterrows()]\n",
    "\n",
    "    \n",
    "    \n",
    "combinations = pd.merge(dataset, model, on=[\"neuron_n\", \"image_n\"]).sort_values(['neuron_n', 'image_n'])\n",
    "   \n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': 'd41d8cd98f00b204e9800998ecf8427e',\n",
    "               'trainer_fn': 'nnsysident.training.trainers.standard_trainer',\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]\n",
    "\n",
    "#Experiments.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "#Experiments.Restrictions.insert(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (SameNI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transfer entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_experiment_name = 'SIM, Direct, se2d_fullgaussian2d, 0-0-3'\n",
    "overall_best = False\n",
    "\n",
    "for entry in get_transfer_entries(old_experiment_name = old_experiment_name, overall_best=overall_best):\n",
    "    Transfer().add_entry(**entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for experiment\n",
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static0-0-3-preproc0.zip']\n",
    "\n",
    "experiment_name = 'SIM, core_transfer (sameNI), {}, 0-0-3 -> 0-0-3'.format(model_name)\n",
    "experiment_comment = 'Transer training on simulated data with {} and static_loaders of static0-0-3-preproc0.zip. Varying number of neurons and images in the transfer core.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "trainer_fn = \"nnsysident.training.trainers.standard_trainer\"\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "\n",
    "trainer = pd.DataFrame((Trainer & 'trainer_fn=\"{}\"'.format(trainer_fn)).fetch())\n",
    "trainer = pd.concat([trainer, trainer['trainer_config'].apply(pd.Series)], axis = 1).drop('trainer_config', axis = 1)\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [int(find_number(row.model_comment, arg + '=')[0]) for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1).drop('model_config', axis = 1)\n",
    "\n",
    "\n",
    "# Restrict here\n",
    "model = model\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "dataset = dataset.loc[~ dataset['exclude_neuron_n'].isnull()]\n",
    "trainer = trainer.loc[trainer['detach_core'] == True]\n",
    "\n",
    "\n",
    "old_experiment_name = 'SIM, Direct, {}, 0-0-3'.format(model_name)\n",
    "transfer = get_transfer(old_experiment_name)\n",
    "transfer = transfer.rename(columns = {'t_neuron_base_seed': 'neuron_base_seed', \n",
    "                                      't_image_base_seed': 'image_base_seed', \n",
    "                                      't_neuron_n': 'neuron_n', \n",
    "                                      't_image_n':'image_n'})\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"neuron_n\", \"image_n\"]).sort_values(['neuron_n', 'image_n'])\n",
    "combinations = pd.merge(combinations, transfer, on=['neuron_base_seed',\n",
    "                                                    'image_base_seed', \n",
    "                                                    'neuron_n',\n",
    "                                                    'image_n'])\n",
    "\n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': trainer['trainer_hash'].values[0],\n",
    "               'trainer_fn': trainer['trainer_fn'].values[0],\n",
    "               'transfer_hash': row['transfer_hash'], \n",
    "               \"transfer_fn\": row['transfer_fn'],\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperimentsTransfer.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "ExperimentsTransfer.Restrictions.insert(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dataset entries for different neuron and image seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_base_seeds = [1,2,3]\n",
    "image_base_seeds = [1,2,3]\n",
    "\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "paths = ['data/static0-0-3-preproc0.zip']\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[(dataset['neuron_base_seed'] == 1) & \n",
    "                      (dataset['image_base_seed'] == 1) & \n",
    "                      [np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "dataset = dataset.loc[~ dataset['exclude_neuron_n'].isnull()]\n",
    "\n",
    "for loc, row in dataset.iterrows():\n",
    "    for neuron_base_seed in neuron_base_seeds:\n",
    "        for image_base_seed in image_base_seeds:\n",
    "            dataset_config = row['dataset_config']\n",
    "            dataset_config.update(neuron_base_seed=neuron_base_seed, image_base_seed=image_base_seed)\n",
    "            Dataset().add_entry(dataset_fn=row['dataset_fn'], \n",
    "                                dataset_config=dataset_config, \n",
    "                                dataset_fabrikant=row['dataset_fabrikant'], \n",
    "                                dataset_comment=row['dataset_comment'], skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transfer entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_experiment_name = 'SIM, Direct, se2d_fullgaussian2d, 0-0-3'\n",
    "overall_best = True\n",
    "\n",
    "for entry in get_transfer_entries(old_experiment_name = old_experiment_name, overall_best=overall_best):\n",
    "    Transfer().add_entry(**entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'se2d_spatialxfeaturelinear'\n",
    "paths = ['data/static0-0-3-preproc0.zip']\n",
    "\n",
    "\n",
    "experiment_name = 'SIM, core_transfer (best), {}, 0-0-3 -> 0-0-3'.format(model_name)\n",
    "experiment_comment = 'Directly trained on simulated data with {} and static_loaders of static0-0-3-preproc0.zip. Varying number of neurons and images.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "trainer_fn = \"nnsysident.training.trainers.standard_trainer\"\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "\n",
    "trainer = pd.DataFrame((Trainer & 'trainer_fn=\"{}\"'.format(trainer_fn)).fetch())\n",
    "trainer = pd.concat([trainer, trainer['trainer_config'].apply(pd.Series)], axis = 1).drop('trainer_config', axis = 1)\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [int(find_number(row.model_comment, arg + '=')[0]) for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1).drop('model_config', axis = 1)\n",
    "\n",
    "\n",
    "# Restrict here\n",
    "model = model\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "dataset = dataset.loc[~ dataset['exclude_neuron_n'].isnull()]\n",
    "trainer = trainer.loc[trainer['detach_core'] == True]\n",
    "\n",
    "\n",
    "old_experiment_name = 'SIM, Direct, {}, 0-0-3'.format(model_name)\n",
    "transfer = get_transfer(old_experiment_name)\n",
    "transfer = transfer.loc[(transfer['t_neuron_n'] == 1000) & (transfer['t_image_n'] == 4000)]\n",
    "transfer = transfer.rename(columns = {'t_neuron_base_seed': 'neuron_base_seed', 't_image_base_seed': 'image_base_seed'})\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"neuron_n\", \"image_n\"]).sort_values(['neuron_n', 'image_n'])\n",
    "combinations = pd.merge(combinations, transfer, on=['neuron_base_seed', 'image_base_seed'])\n",
    "\n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': trainer['trainer_hash'].values[0],\n",
    "               'trainer_fn': trainer['trainer_fn'].values[0],\n",
    "               'transfer_hash': row['transfer_hash'], \n",
    "               \"transfer_fn\": row['transfer_fn'],\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExperimentsTransfer.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "ExperimentsTransfer.Restrictions.insert(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add single entries in main tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = dict(dataset_fn='nnsysident.datasets.mouse_loaders.static_loaders', \n",
    "             dataset_config = dict(paths=paths,\n",
    "                                   batch_size=64,\n",
    "                                   seed=1),\n",
    "             dataset_comment='full dataset',\n",
    "             dataset_fabrikant='kklurz')\n",
    "#Dataset().add_entry(**entry)\n",
    "\n",
    "entry = dict(model_fn='nnsysident.models.models.se2d_fullgaussian2d', \n",
    "             model_config = dict(),\n",
    "             model_comment='default model',\n",
    "             model_fabrikant='kklurz')\n",
    "#Model().add_entry(**entry)\n",
    "\n",
    "entry = dict(trainer_fn='nnsysident.training.trainers.standard_trainer', \n",
    "             trainer_config = dict(detach_core=True),\n",
    "             trainer_comment='default trainer',\n",
    "             trainer_fabrikant='kklurz')\n",
    "#Trainer().add_entry(**entry)\n",
    "\n",
    "entry = dict(transfer_fn='nnsysident.models.transfer_functions.core_transfer', \n",
    "             transfer_config = dict(t_model_hash=\"d41d8cd98f00b204e9800998ecf8427e\", t_dataset_hash='6fa162a20053a013ab4bd31a21950d35', t_trainer_hash='d41d8cd98f00b204e9800998ecf8427e'),\n",
    "             transfer_comment='test transfer',\n",
    "             transfer_fabrikant='kklurz')\n",
    "#Transfer().add_entry(**entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dataset entries for #images and #neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(Dataset.fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "dataset = dataset.loc[dataset['exclude_neuron_n'].isnull()]\n",
    "\n",
    "for loc, row in dataset.iterrows():\n",
    "    exclude_neuron_n = 3000\n",
    "    entry = dict(dataset_fn='nnsysident.datasets.mouse_loaders.static_loaders', \n",
    "                 dataset_config = dict(paths=row.paths,\n",
    "                                       batch_size=64,\n",
    "                                       neuron_n = row.neuron_n,\n",
    "                                      neuron_base_seed = row.neuron_base_seed,\n",
    "                                      image_n = row.image_n,\n",
    "                                      image_base_seed = row.image_base_seed,\n",
    "                                      exclude_neuron_n = exclude_neuron_n),\n",
    "                 dataset_comment='neuron_n={}, image_n={}, exclude_neuron_n={}'.format(row.neuron_n, row.image_n, exclude_neuron_n),\n",
    "                 dataset_fabrikant='kklurz')\n",
    "    #Dataset().add_entry(**entry, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model state dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(state_dict_1, state_dict_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(state_dict_1.items(), state_dict_2.items()):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if (key_item_1[0] == key_item_2[0]):\n",
    "                print('Mismtach found at', key_item_1[0])\n",
    "            else:\n",
    "                raise Exception\n",
    "    if models_differ == 0:\n",
    "        print('Models match perfectly! :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add failed Bayesian to Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = \"nnsysident.models.models.se2d_pointpooled\"\n",
    "\n",
    "all_info = pd.DataFrame((TrainedModelBayesian * ModelBayesian * DatasetBayesian & 'model_fn = \"{}\"'.format(model_fn)).fetch())\n",
    "all_info = pd.concat([all_info, all_info['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "all_info = pd.concat([all_info, all_info['model_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "for neuron_n in [100, 1000]:\n",
    "    for image_n in [50, 100, 200, 500, 1000, 4000]:\n",
    "        one_exp = all_info.loc[(all_info['neuron_n'] == neuron_n) & (all_info['image_n'] == image_n) & (~ all_info['hidden_kern'].isnull())].sort_values('score')\n",
    "        best = one_exp.loc[one_exp['score'] == one_exp['score'].max()]\n",
    "        print(len(one_exp))\n",
    "        print(one_exp['neuron_n'].values[0], one_exp['image_n'].values[0])\n",
    "#         Model().add_entry(model_fn=best['model_fn'].values[0],\n",
    "#                           model_config=best['model_config'].values[0],\n",
    "#                           model_fabrikant='kklurz',\n",
    "#                           model_comment='{}, neuron_n={}, image_n={}'.format(best['model_fn'].values[0].split('.')[-1], \n",
    "#                                                                              best['neuron_n'].values[0], best['image_n'].values[0]), skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = \"nnsysident.models.models.se2d_spatialxfeaturelinear\"\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [int(find_number(row.model_comment, arg + '=')[0]) for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1).drop('model_config', axis = 1).sort_values(['neuron_n', 'image_n'])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_exp_h = one_exp.set_index('score', drop=False).copy()\n",
    "cols = ['score', 'gamma_readout', 'gamma_input', 'init_mu_range', 'init_sigma']\n",
    "hip.Experiment.from_dataframe(one_exp_h[cols]).display(force_full_width=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
