{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datajoint as dj\n",
    "dj.config['database.host'] = os.environ['DJ_HOST']\n",
    "dj.config['database.user'] = os.environ['DJ_USER']\n",
    "dj.config['database.password'] = os.environ['DJ_PASS']\n",
    "dj.config['enable_python_native_blobs'] = True\n",
    "dj.config['display.limit'] = 200\n",
    "\n",
    "name = 'realdata' \n",
    "dj.config['schema_name'] = f\"konstantin_nnsysident_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import hiplot as hip\n",
    "import statsmodels\n",
    "\n",
    "import nnfabrik\n",
    "from nnfabrik.main import *\n",
    "from nnfabrik import builder\n",
    "from nnfabrik.utility.hypersearch import Bayesian\n",
    "from nnsysident.utility.measures import get_correlations\n",
    "\n",
    "from nnsysident.tables.experiments import *\n",
    "from nnsysident.tables.bayesian import *\n",
    "from nnsysident.datasets.mouse_loaders import static_shared_loaders\n",
    "from nnsysident.datasets.mouse_loaders import static_loaders\n",
    "from nnsysident.tables.scoring import OracleScore, OracleScoreTransfer\n",
    "\n",
    "from nnsysident.datasets.mouse_loaders import static_loader\n",
    "\n",
    "def find_number(text, c):\n",
    "    number_list = re.findall(r'%s(\\d+)' % c, text)\n",
    "    if len(number_list) == 0:\n",
    "        number = None\n",
    "    elif len(number_list) == 1:\n",
    "        number = int(number_list[0])\n",
    "    else:\n",
    "        raise ValueError('More than one number found..') \n",
    "    return number\n",
    "\n",
    "def get_transfer(transfer_hashes):\n",
    "    # prepare the Transfer table in a way that all the info about the transferred model is in the DataFrame. Just pd.merge (on transfer_fn and transfer_hash)\n",
    "    # it then with the model that the transferred model was used for. \n",
    "    \n",
    "    transfer = pd.DataFrame((Transfer & 'transfer_hash in {}'.format(tuple(transfer_hashes))).fetch())\n",
    "    transfer = pd.concat([transfer, transfer['transfer_config'].apply(pd.Series)], axis = 1).drop('transfer_config', axis = 1)\n",
    "\n",
    "    restriction = transfer.rename(columns = {'t_model_hash': 'model_hash', 't_dataset_hash': 'dataset_hash', 't_trainer_hash': 'trainer_hash'})            \n",
    "    restriction = restriction[['model_hash', 'dataset_hash', 'trainer_hash']].to_dict('records')\n",
    "\n",
    "    tm = pd.DataFrame((TrainedModel * Dataset * Seed & restriction).fetch()).rename(\n",
    "        columns = {'model_hash': 't_model_hash', 'trainer_hash': 't_trainer_hash', 'dataset_hash': 't_dataset_hash'})               \n",
    "    tm = tm.sort_values('score', ascending=False).drop_duplicates(['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "\n",
    "    transfer = pd.merge(transfer, tm, how='inner', on=['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "    transfer = pd.concat([transfer, transfer['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "    transfer.columns = ['t_' + col if col[:2] != 't_' and col[:8] != 'transfer'  else col for col in transfer.columns]\n",
    "    transfer = transfer.sort_values(['t_multi_match_n', 't_image_n', 't_multi_match_base_seed', 't_image_base_seed'])\n",
    "    return transfer\n",
    "\n",
    "\n",
    "def get_transfer_entries(old_experiment_name, overall_best):\n",
    "    tm = pd.DataFrame((TrainedModel * Dataset * Seed * Experiments.Restrictions & 'experiment_name=\"{}\"'.format(old_experiment_name)).fetch())\n",
    "    tm = pd.concat([tm, tm['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "\n",
    "    model_fn = np.unique(tm['model_fn'])\n",
    "    assert len(model_fn) == 1 ,\"Must have exactly 1 model function in experiment\"\n",
    "    model_fn = model_fn[0] \n",
    "\n",
    "    # Filter out best model(s) \n",
    "    if overall_best is True:\n",
    "        tm = tm.loc[(tm['multi_match_n'] == tm['multi_match_n'].max()) & (tm['image_n'] == tm['image_n'].max())]\n",
    "    tm = tm.sort_values('score', ascending=False).drop_duplicates(['multi_match_n', 'image_n', 'multi_match_base_seed', 'image_base_seed']).sort_values(['multi_match_n', 'image_n'])\n",
    "\n",
    "    # make entries for Trasfer table\n",
    "    entries = [dict(transfer_fn='nnsysident.models.transfer_functions.core_transfer', \n",
    "                     transfer_config = dict(t_model_hash=row.model_hash, t_dataset_hash=row.dataset_hash, t_trainer_hash=row.trainer_hash),\n",
    "                     transfer_comment=model_fn.split('.')[-1] + ', multi_match_n={}, multi_match_base_seed={}, image_n={}, image_base_seed={}'.format(row.multi_match_n, \n",
    "                                                                                                                                    row.multi_match_base_seed, \n",
    "                                                                                                                                    row.image_n, \n",
    "                                                                                                                                    row.image_base_seed),\n",
    "                     transfer_fabrikant='kklurz') for loc, row in tm.iterrows()]\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = pd.DataFrame()\n",
    "for experiment_name in [\"Real, core_transfer (best), se2d_fullgaussian2d, 4-set -> 4-set\", \n",
    "                        \"Real, core_transfer (best from GAUSSIAN!), se2d_spatialxfeaturelinear, 4-set -> 4-set\"]:\n",
    "    \n",
    "    data_ = pd.DataFrame((TrainedModelTransfer * Dataset * Model * Trainer * Seed * Transfer.proj() * OracleScoreTransfer * ExperimentsTransfer.Restrictions \n",
    "                          & 'experiment_name=\"{}\"'.format(experiment_name)).fetch())\n",
    "    transfer_hashes = list(data_.transfer_hash)\n",
    "    transfer_ = get_transfer(transfer_hashes)\n",
    "    data_ = pd.merge(data_, transfer_, how='inner', on=['transfer_hash', 'transfer_fn'])\n",
    "    data = pd.concat([data, data_])\n",
    "data['readout'] = [row.model_fn.split('.')[-1][5:] for loc, row in data.iterrows()]\n",
    "data = pd.concat([data, data['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "data['image_n'] = data['image_n'] * 4\n",
    "data = data.rename(columns = {'multi_match_n': '# neurons', 'image_n': \"# images\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = \"nnsysident.models.models.se2d_fullgaussian2d\"\n",
    "my_data = data.loc[(data['# images'] == 200) & (data['multi_match_base_seed'] == 1) & (data['model_fn'] == model_fn)]\n",
    "key = dict(model_fn=my_data.model_fn.values[0], \n",
    "           model_hash=my_data.model_hash.values[0],\n",
    "           dataset_hash=my_data.dataset_hash.values[0], \n",
    "           trainer_hash = my_data.trainer_hash.values[0],\n",
    "           transfer_hash = my_data.transfer_hash.values[0])\n",
    "(TrainedModelTransfer & key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary stuff\n",
    "from torch.nn import functional as F\n",
    "from neuralpredictors.measures import PoissonLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_model(image_n):\n",
    "    model_fn = \"nnsysident.models.models.se2d_fullgaussian2d\"\n",
    "    my_data = data.loc[(data['# images'] == image_n) & (data['multi_match_base_seed'] == 1) & (data['model_fn'] == model_fn)]\n",
    "    key = dict(model_fn=my_data.model_fn.values[0], \n",
    "           model_hash=my_data.model_hash.values[0],\n",
    "           dataset_hash=my_data.dataset_hash.values[0], \n",
    "           trainer_hash = my_data.trainer_hash.values[0],\n",
    "           transfer_hash = my_data.transfer_hash.values[0])\n",
    "    key = (TrainedModelTransfer & key).proj().fetch1()\n",
    "    dataloaders, model = TrainedModelTransfer().load_model(key=key)  \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model, dataloaders\n",
    "\n",
    "def loss(model, data_key, neuron_idx, shift, target, core_out):\n",
    "    x = model.readout[data_key](core_out, out_idx=neuron_idx, shift=shift)\n",
    "    output = F.elu(x) + 1\n",
    "    loss_scale = np.sqrt(len(dataloaders['train'][data_key].dataset) / core_out.shape[0])\n",
    "    loss = loss_scale * criterion(output, target).detach().cpu().numpy()\n",
    "    return loss\n",
    "\n",
    "# This is the neuron we want to look at\n",
    "neuron_idx = [569]\n",
    "criterion = PoissonLoss(avg=False)\n",
    "data_key = '22564-3-12-0'\n",
    "\n",
    "# compute output of the core to images. Unfortunately have to break after some time because the memory isn't big enough for the whole thing\n",
    "model, dataloaders = get_model(image_n=200) # 17596\n",
    "core_outs = []\n",
    "targets = []\n",
    "\n",
    "# for i, (data_x, data_y) in enumerate(tqdm(dataloaders['train'][data_key])):\n",
    "#     x = model.core(data_x)\n",
    "#     core_outs.append(x)\n",
    "#     targets.append(data_y)\n",
    "#     break\n",
    "    \n",
    "for i, (data_x, data_y) in enumerate(tqdm(dataloaders['train'][data_key])):\n",
    "    x = model.core(data_x).cpu().data.numpy()\n",
    "    core_outs.append(x)\n",
    "    targets.append(data_y.cpu().data.numpy())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'inshallahmodel.mojo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_outs = np.concatenate(core_outs)\n",
    "targets = np.concatenate(targets).T\n",
    "feature_weights = model.readout[data_key].features.squeeze().T.cpu().data.numpy()\n",
    "biases = model.readout[data_key].bias.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_outs.shape, targets.shape, feature_weights.shape, biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_spatial_scale(x_init, init_range, final_range):\n",
    "    return (x_init - min(init_range)) * (max(final_range - min(final_range)) / max(init_range - min(init_range))) + min(final_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu1(val, alpha=1.):\n",
    "    elued_val = val.copy()\n",
    "    elued_val[elued_val<=0] = alpha * (np.exp(elued_val[elued_val<=0]) - 1)\n",
    "    return elued_val + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output, target, bias=1e-12, avg=False):\n",
    "    loss = output - target * np.log(output + bias)\n",
    "    return loss.mean() if avg else loss.sum(axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx = 809\n",
    "\n",
    "feature_weight_expanded = feature_weights[neuron_idx][None, :, None, None]\n",
    "out = (feature_weight_expanded * core_outs).sum(axis=1, keepdims=True) + biases[neuron_idx]\n",
    "out = elu1(out)\n",
    "mask = loss_fn(out, targets[neuron_idx][:, None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_y, grid_x = model.readout[data_key].mu.squeeze()[np.array(neuron_idx)].squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_y, feature_map_x = transform_spatial_scale(grid_y, np.array([-1, 1]), np.array([0, 50])), transform_spatial_scale(grid_x, np.array([-1, 1]), np.array([0, 22]))\n",
    "tx, ty = np.where(mask == mask.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.imshow(mask)\n",
    "ax.scatter(feature_map_y, feature_map_x, marker='x', color='r', s=100)\n",
    "ax.scatter(ty, tx, marker='x', color='orange', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_indices_raw = np.array([3511, 3105, 3036, 3751])\n",
    "final_indices = final_indices_raw - (985 + 979 + 978)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, dpi=150, figsize=(4, 8), sharex=True)\n",
    "\n",
    "for ind, (neuron_idx, ax) in enumerate(zip(final_indices, axes)):\n",
    "    feature_weight_expanded = feature_weights[neuron_idx][None, :, None, None]\n",
    "    out = (feature_weight_expanded * core_outs).sum(axis=1, keepdims=True) + biases[neuron_idx]\n",
    "    out = elu1(out)\n",
    "    mask = loss_fn(out, targets[neuron_idx][:, None, None, None])\n",
    "    \n",
    "    grid_y, grid_x = model.readout[data_key].mu.squeeze()[np.array(neuron_idx)].squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    feature_map_y, feature_map_x = transform_spatial_scale(grid_y, np.array([-1, 1]), np.array([0, 50])), transform_spatial_scale(grid_x, np.array([-1, 1]), np.array([0, 22]))\n",
    "    tx, ty = np.where(mask == mask.min())\n",
    "    \n",
    "    print(mask.min(), mask.max())\n",
    "    \n",
    "    ax.imshow(mask, vmin=30.969355, vmax=43.1981)\n",
    "    ax.scatter(feature_map_y, feature_map_x, marker='x', color='r', s=100, label='Learned position')\n",
    "    ax.scatter(ty, tx, marker='x', color='orange', s=100, label='Position with minimum value')\n",
    "    \n",
    "    if ind == 0:\n",
    "        leg = ax.legend(edgecolor=\"None\", facecolor=\"None\", loc=\"upper left\")\n",
    "        for text in leg.get_texts():\n",
    "            text.set_color(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data['# images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, dataloaders = get_model(image_n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = model.readout['22564-2-12-0'].grid.squeeze().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*grid.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grids = []\n",
    "for image_n, ax in zip(np.unique(data['# images']), axes.flat):\n",
    "    model, dataloaders = get_model(image_n=image_n)\n",
    "    grid = model.readout[data_key].grid.squeeze().cpu().data.numpy()\n",
    "    all_grids.append(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, len(np.unique(data['# images'])) // 2, dpi=200, figsize=(6, 4.3))\n",
    "for ind, ax in enumerate(axes.flat):\n",
    "\n",
    "    ax.scatter(*all_grids[ind].T, color='navy', s=.1, label='Learned position')\n",
    "    ax.set(xlim=(-1.1, 1.1), ylim=(-1.1, 1.1))\n",
    "    sns.despine(top=True, ax=ax)\n",
    "    \n",
    "    ax.set(xticks=[-1, -.5, 0, .5, 1])\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(ls='--')\n",
    "    if ind != 3:\n",
    "        ax.set(xticklabels=[], yticklabels=[])\n",
    "        \n",
    "    ax.set_title(f\"# images/scan: {np.unique(data['# images'])[ind] // 4}\", fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"grids_sharedlim.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(np.unique(data['# images'])), dpi=200, figsize=(13, 2))\n",
    "for ind, ax in enumerate(axes.flat):\n",
    "\n",
    "    ax.scatter(*all_grids[ind].T, marker='o', color='darkorange', s=.1, label='Learned position')\n",
    "#     ax.set(xlim=(-1.1, 1.1), ylim=(-1.1, 1.1))\n",
    "    sns.despine(top=True, ax=ax)\n",
    "    \n",
    "#     ax.set(xticks=[-1, -.5, 0, .5, 1])\n",
    "#     if ind > 0:\n",
    "#         ax.set(xticklabels=[], yticklabels=[])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"grids.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_spatial_scale(x_init, init_range, final_range):\n",
    "    return (x_init - min(init_range)) * (max(final_range - min(final_range)) / max(init_range - min(init_range))) + min(final_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spatial_scale(0.16196062, np.array([-1, 1]), np.array([0, 50])), transform_spatial_scale(0.20249286, np.array([-1, 1]), np.array([0, 22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask[7:-7, 15:-15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = np.where(mask == mask.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)\n",
    "plt.scatter(29.049015499999996-15, 13.22742146-7, marker='x', color='r', s=100)\n",
    "plt.scatter(ty, tx, marker='x', color='orange', s=100)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = core_outs[0][1].cpu().data.numpy()\n",
    "feats = core_outs[0].mean(dim=0).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(20, 15))\n",
    "\n",
    "for feat, ax in zip(feats, axes.flat):\n",
    "    ax.imshow(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.imshow(feats.mean(axis=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.readout['22564-2-12-0'].mu_transform[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = nn.Linear(2, 2, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.weight.data = torch.tensor([[0., -1.], \n",
    "                               [1., 0.]]) #, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.array([[2, 2]]).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, dataloaders = get_model(image_n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_lambda = .1\n",
    "basis_ortho_reg = torch.mm(model.readout['22564-3-12-0'].mu_transform[0].weight[:, 0].view(1, -1), \n",
    "                           model.readout['22564-3-12-0'].mu_transform[0].weight[:, 1].view(1, -1).T).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_lambda * basis_ortho_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.readout['22564-3-12-0'].mu_transform[0].weight.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = model.readout['22564-3-12-0'].mu_transform[0].weight.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, A[0, 0]], [0, A[1, 0]], 'r')\n",
    "plt.plot([0, A[0, 1]], [0, A[1, 1]], 'r')\n",
    "plt.plot([0, B[0, 0]], [0, B[1, 0]], 'k')\n",
    "plt.plot([0, B[0, 1]], [0, B[1, 1]], 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
