{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "- Copy any existing hdf5 file. Do NOT use the original, the content of the file will be overwritten!\n",
    "- Rename it according to: static\\<animal_id>-\\<session>-\\<scan_idx>-preproc0.h5\n",
    "    - animal_id should be fixed to 0 for simulated datasets\n",
    "    - session should indicate datasets with the same way of general noise generation , i.e. independent poisson, noise correlations, brainstate simulations...\n",
    "    - scan_idx should indicate datasets with different neurons, i.e. different parameter values for the (gabor) filter generation\n",
    "- Run all cells (\"Analyze simulated data\" not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from functools import partial\n",
    "from mlutils.data.datasets import StaticImageSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the session and scan_idx of your simulated data. Animal_id must always be 0 for simulated data!\n",
    "animal_id = 0\n",
    "session = 1\n",
    "scan_idx= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hdf5 file\n",
    "path = 'static{}-{}-{}-preproc0.h5'.format(animal_id, session, scan_idx)\n",
    "f = h5py.File(path, 'r+')\n",
    "\n",
    "# Add keys to store parameters of simulated data\n",
    "try:\n",
    "    f['simulation_info/ground_truths'] = np.full_like(f['responses'][:], None)\n",
    "    f['simulation_info/theta'] = np.full((len(f['neurons']['unit_ids'][:])), None).astype('<f8')    \n",
    "    f['simulation_info/sigma'] = np.full((len(f['neurons']['unit_ids'][:])), None).astype('<f8')\n",
    "    f['simulation_info/Lambda'] = np.full((len(f['neurons']['unit_ids'][:])), None).astype('<f8')\n",
    "    f['simulation_info/psi'] = np.full((len(f['neurons']['unit_ids'][:])), None).astype('<f8') \n",
    "    f['simulation_info/gamma'] = np.full((len(f['neurons']['unit_ids'][:])), None).astype('<f8')\n",
    "    f['simulation_info/center'] = np.full((len(f['neurons']['unit_ids'][:]), 2), None).astype('<f8')\n",
    "    f['simulation_info/size'] = np.full((len(f['neurons']['unit_ids'][:]), 2), None).astype('<f8')\n",
    "    f['simulation_info/normalize'] = np.full((len(f['neurons']['unit_ids'][:])), None).astype('<f8')\n",
    "    f['simulation_info/seed'] = np.full((1), None).astype('<f8')\n",
    "except: \n",
    "    print('Error occurred trying to add keys')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_fn(theta, sigma=2, Lambda=10, psi=np.pi/2, gamma=.8, center=(0, 0), size=(36, 64), normalize=True):\n",
    "    \"\"\"Returns a gabor filter.\n",
    "\n",
    "    Args:\n",
    "        theta (float): Orientation of the sinusoid (in ratian).\n",
    "        sigma (float): std deviation of the Gaussian.\n",
    "        Lambda (float): Sinusoid wavelengh (1/frequency).\n",
    "        psi (float): Phase of the sinusoid.\n",
    "        gamma (float): The ratio between sigma in x-dim over sigma in y-dim (acts\n",
    "            like an aspect ratio of the Gaussian).\n",
    "        center (tuple of integers): The position of the filter.\n",
    "        size (tuple of integers): Image height and width.\n",
    "        normalize (bool): Whether to normalize the entries. This is computed by\n",
    "            dividing each entry by the root sum squared of the whole image.\n",
    "\n",
    "    Returns:\n",
    "        2D Numpy array: A gabor filter.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sigma_x = sigma\n",
    "    sigma_y = sigma / gamma\n",
    "\n",
    "    xmax, ymax = size\n",
    "    xmax, ymax = (xmax - 1)/2, (ymax - 1)/2\n",
    "    xmin = -xmax\n",
    "    ymin = -ymax\n",
    "    (y, x) = np.meshgrid(np.arange(ymin, ymax+1), np.arange(xmin, xmax+1))\n",
    "\n",
    "    # shift the positon\n",
    "    y -= center[0]\n",
    "    x -= center[1]\n",
    "\n",
    "    # Rotation\n",
    "    x_theta = x * np.cos(theta) + y * np.sin(theta)\n",
    "    y_theta = -x * np.sin(theta) + y * np.cos(theta)\n",
    "\n",
    "    gb = np.exp(-.5 * (x_theta ** 2 / sigma_x ** 2 + y_theta ** 2 / sigma_y ** 2)) * np.cos(2 * np.pi / Lambda * x_theta + psi)\n",
    "\n",
    "    if normalize:\n",
    "        # root sum squared\n",
    "        gb /= np.sqrt(np.sum(gb ** 2))\n",
    "        # make sure the sum is equal to zero\n",
    "        # gb[gb > 0] = gb[gb > 0] * (np.abs(gb[gb < 0].sum()) / gb[gb > 0].sum())\n",
    "        gb -= gb.mean()\n",
    "\n",
    "    return gb\n",
    "\n",
    "def _elu(val, alpha=1.):\n",
    "    return val if val > 0 else alpha * (np.exp(val) - 1)\n",
    "elu1 = lambda arr, alpha=1.: np.array(list(map(partial(_elu, alpha=alpha), arr))) + 1\n",
    "\n",
    "def compute_linear_out(img, filters):\n",
    "    out = img * filters\n",
    "    return out.sum(axis=(1, 2))\n",
    "\n",
    "def compute_statistics(data):\n",
    "    return np.max(data, axis=0), np.mean(data, axis=0), np.median(data, axis=0), np.min(data, axis=0), np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create gabor filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = len(f['neurons']['unit_ids'][:])\n",
    "\n",
    "# leave 10 pixes space between the center of the Gabor and the edge of the image\n",
    "# X dimension: 64\n",
    "xs = np.random.randint(low=-22, high=23, size=n_neurons)\n",
    "# Y dimension: 32\n",
    "ys = np.random.randint(low=-8, high=9, size=n_neurons)\n",
    "center = np.vstack([xs,ys]).T\n",
    "\n",
    "# Angles and phases range from 0 to 2 pi\n",
    "theta = np.random.uniform(low=0, high=2, size=n_neurons) * np.pi\n",
    "psi = np.random.uniform(low=0, high=2, size=n_neurons) * np.pi\n",
    "\n",
    "# keep like default \n",
    "sigma = np.full((n_neurons), 2)\n",
    "Lambda = np.full((n_neurons), 10)\n",
    "gamma = np.full((n_neurons), .8)\n",
    "size = np.full((n_neurons, 2), np.squeeze(f['images'][0]).shape)\n",
    "normalize = np.full((n_neurons), True)\n",
    "\n",
    "filters = []\n",
    "for n in range(n_neurons):\n",
    "    Filter = gabor_fn(theta=theta[n], sigma=sigma[n], Lambda=Lambda[n], psi=psi[n], gamma=gamma[n], center=center[n], size=size[n], normalize=normalize[n])\n",
    "    filters.append(Filter)\n",
    "filters = np.array(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 3 example filters\n",
    "for i, gabor in enumerate(filters[:3]):\n",
    "    plt.imshow(gabor, vmin=-np.abs(gabor.max()), vmax=np.abs(gabor.max()), cmap=plt.cm.bwr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to, create random images based on images statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes long. Pre-fitted distribution in the next cell. \n",
    "\n",
    "# beta = stats.beta\n",
    "\n",
    "# pixels = f['images'][:].flatten()\n",
    "# y = pixels\n",
    "\n",
    "# x = np.linspace(0, y.max(), len(y))\n",
    "# # fit\n",
    "# param = beta.fit(y)\n",
    "# pdf_fitted = beta.pdf(x, *param)\n",
    "# plt.plot(x, pdf_fitted, color='r')\n",
    "\n",
    "# # plot the histogram\n",
    "# plt.hist(y, normed=True, bins=len(np.unique(pixels)))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new images from distribution\n",
    "params = (1.595, 2.203, -2.265, 272.882) # fitted beta distribution params for pixels of mouse static natural images \n",
    "fitted_beta = stats.beta(*params)\n",
    "\n",
    "beta_images = np.zeros(f['images'][:].shape)\n",
    "for i, image_id in enumerate(f['item_info']['frame_image_id'][:]):\n",
    "    # Same image_id gets the same random image by setting the random seed to the same value for same image_id\n",
    "    random_state = np.random.get_state()\n",
    "    np.random.seed(image_id)\n",
    "    \n",
    "    beta_images[i][0] = fitted_beta.rvs(beta_images[i][0].shape)\n",
    "    \n",
    "    np.random.set_state(random_state)\n",
    "\n",
    "# Force into pixel interval and round to integers \n",
    "maximum = f['images'][:].max()\n",
    "minimum = f['images'][:].min()\n",
    "beta_images[beta_images > maximum] = maximum\n",
    "beta_images[beta_images < minimum] = minimum\n",
    "beta_images = np.round(beta_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the input images for the simulated data\n",
    "image_source = 'beta distribution'\n",
    "\n",
    "if image_source == 'old_dataset':\n",
    "    images = f['images'][:]   # use the images that were in the initial hdf5 file\n",
    "elif image_source == 'beta distribution':\n",
    "    images = beta_images        # use the images drawn from the beta distribution\n",
    "else:\n",
    "    raise ValueError('Image source not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=0.3 # scale the responses to be in the range of real data. This parameter can be adjusted freely\n",
    "\n",
    "ground_truths= []\n",
    "for image in tqdm(images):\n",
    "    linear_out = compute_linear_out(img=image, filters=filters) * scale\n",
    "    ground_truth = elu1(linear_out)    \n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "ground_truths = np.array(ground_truths)\n",
    "responses = np.random.poisson(ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analize simulated data (not necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Responses of one neuron to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StaticImageSet('/notebooks/data/static20457-5-9-preproc0.h5', \"images\", \"responses\")\n",
    "dat_responses = dataset.responses.copy()\n",
    "print('Maximum response in simulated data: {}'.format(responses.max()))\n",
    "print('Maximum response in real data: {}'.format(dat_responses.max()))\n",
    "\n",
    "# s = np.array(dataset.statistics[\"responses\"]['all'][\"std\"])\n",
    "\n",
    "# threshold = 0.01 * s.mean()\n",
    "# idx = s > threshold\n",
    "# _response_precision = np.ones_like(s) / threshold\n",
    "# _response_precision[idx] = 1 / s[idx]\n",
    "\n",
    "# dat_responses = (dat_responses * _response_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5), dpi=150)\n",
    "\n",
    "axes[0].plot(responses[:,0])\n",
    "axes[0].set_title('Simulated Neuron')\n",
    "axes[0].set_xlabel('Image')\n",
    "\n",
    "axes[1].plot(dat_responses[:,0])\n",
    "axes[1].set_title('Real Neuron')\n",
    "axes[1].set_xlabel('Image')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5), dpi=150)\n",
    "\n",
    "axes[0].plot(responses[dataset.tiers=='test'][:,0])\n",
    "axes[0].set_title('Simulated Neuron')\n",
    "axes[0].set_xlabel('Image')\n",
    "\n",
    "axes[1].plot(dat_responses[dataset.tiers=='test'][:,0])\n",
    "axes[1].set_title('Real Neuron')\n",
    "axes[1].set_xlabel('Image')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5), dpi=150)\n",
    "\n",
    "i=1\n",
    "\n",
    "axes[0].plot(responses[dataset.info.frame_image_id == 2214][:,i])\n",
    "axes[0].set_title('Simulated Neuron')\n",
    "axes[0].set_xlabel('Image')\n",
    "\n",
    "axes[1].plot(dat_responses[dataset.info.frame_image_id == 2214][:,i])\n",
    "axes[1].set_title('Real Neuron')\n",
    "axes[1].set_xlabel('Image')\n",
    "plt.tight_layout()                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response of all neurons to one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5), dpi=150)\n",
    "\n",
    "axes[0].plot(responses[0,:])\n",
    "axes[0].set_title('Simulated Neurons')\n",
    "axes[0].set_xlabel('Neuron')\n",
    "\n",
    "axes[1].plot(dat_responses[0,:])\n",
    "axes[1].set_title('Real Neurons')\n",
    "axes[1].set_xlabel('Neuron')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum responses of all neurons to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(5, 3), dpi=150)\n",
    "\n",
    "axes[0].hist(responses.max(axis=0))\n",
    "axes[0].set_title('Simulated Neurons')\n",
    "axes[0].set_xlabel('Max response')\n",
    "\n",
    "axes[1].hist(dat_responses.max(axis=0))\n",
    "axes[1].set_title('Real Neurons')\n",
    "axes[1].set_xlabel('Max response')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite information that does not exist in simulated data\n",
    "f['pupil_center'][:]                         = np.full_like(f['pupil_center'][:], None)\n",
    "f['behavior'][:]                             = np.full_like(f['behavior'], None)\n",
    "f['trial_idx'][:]                            = np.full_like(f['trial_idx'], 0)\n",
    "f['item_info']['trial_idx'][:]               = np.full_like(f['item_info']['trial_idx'][:], 0)\n",
    "f['item_info']['frame_last_flip'][:]         = np.full_like(f['item_info']['frame_last_flip'][:], 0)\n",
    "f['item_info']['frame_pre_blank_period'][:]  = np.full_like(f['item_info']['frame_pre_blank_period'][:], 0)\n",
    "f['item_info']['frame_presentation_time'][:] = np.full_like(f['item_info']['frame_presentation_time'][:], 0)\n",
    "f['item_info']['frame_trial_ts'][:]          = np.full_like(f['item_info']['frame_trial_ts'][:], 0)\n",
    "\n",
    "# Overwrite responses and ground truth\n",
    "f['responses'][:] = responses\n",
    "if image_source == 'old_dataset':\n",
    "    pass\n",
    "elif image_source == 'beta distribution':\n",
    "    f['images'][:] = images\n",
    "else:\n",
    "    raise ValueError('Image source not defined')\n",
    "\n",
    "#Write information that is only present in simulated data\n",
    "f['simulation_info']['ground_truths'][:] = ground_truths\n",
    "f['simulation_info']['theta'][:] = theta  \n",
    "f['simulation_info']['sigma'][:] = sigma\n",
    "f['simulation_info']['Lambda'][:] = Lambda\n",
    "f['simulation_info']['psi'][:] = psi\n",
    "f['simulation_info']['gamma'][:] = gamma\n",
    "f['simulation_info']['center'][:] = center\n",
    "f['simulation_info']['size'][:] = size\n",
    "f['simulation_info']['normalize'][:] = normalize\n",
    "#f['simulation_info']['seed'][:] = seed\n",
    "    \n",
    "# Compute and overwrite statistics\n",
    "for key in f['statistics'].keys():\n",
    "    if not key in ('images',  'responses'):\n",
    "        del f['statistics'][key] \n",
    "        \n",
    "maximum, mean, median, minimum, std = compute_statistics(responses[f['tiers'][:] == b'train']) \n",
    "f['statistics']['responses']['all']['max'][:]    = maximum\n",
    "f['statistics']['responses']['all']['mean'][:]   = mean\n",
    "f['statistics']['responses']['all']['median'][:] = median\n",
    "f['statistics']['responses']['all']['min'][:]    = minimum\n",
    "f['statistics']['responses']['all']['std'][:]    = std\n",
    "\n",
    "maximum, mean, median, minimum, std = compute_statistics(responses[(f['types'][:] == b'stimulus.Frame') & (f['tiers'][:] == b'train')]) \n",
    "f['statistics']['responses']['stimulus.Frame']['max'][:]    = maximum\n",
    "f['statistics']['responses']['stimulus.Frame']['mean'][:]   = mean\n",
    "f['statistics']['responses']['stimulus.Frame']['median'][:] = median\n",
    "f['statistics']['responses']['stimulus.Frame']['min'][:]    = minimum\n",
    "f['statistics']['responses']['stimulus.Frame']['std'][:]    = std\n",
    "\n",
    "# Overwrite dataset information\n",
    "f['neurons']['animal_ids'][:] = np.full_like(f['neurons']['animal_ids'][:], animal_id)\n",
    "f['neurons']['scan_idx'][:]   = np.full_like(f['neurons']['scan_idx'][:], scan_idx)\n",
    "f['neurons']['sessions'][:]   = np.full_like(f['neurons']['sessions'][:], session)\n",
    "\n",
    "f['neurons']['area'][:]       = np.full_like(f['neurons']['area'][:], b'si') #simulated\n",
    "f['neurons']['layer'][:]      = np.full_like(f['neurons']['layer'][:], b'simu') #simulated\n",
    "\n",
    "f['item_info']['animal_id'][:]= np.full_like(f['item_info']['animal_id'][:], animal_id)\n",
    "f['item_info']['session'][:]  = np.full_like(f['item_info']['session'][:], session)\n",
    "f['item_info']['scan_idx'][:] = np.full_like(f['item_info']['scan_idx'][:], scan_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutils.data.datasets import StaticImageSet\n",
    "from torch.utils.data import DataLoader\n",
    "from data.loader_configurators import RepeatsBatchSampler\n",
    "from mlutils.measures import corr\n",
    "\n",
    "dataset = StaticImageSet(path, \"images\", \"responses\")\n",
    "\n",
    "types = np.unique(dataset.types)\n",
    "if len(types) == 1 and types[0] == \"stimulus.Frame\":\n",
    "    condition_hashes = dataset.info.frame_image_id\n",
    "else:\n",
    "    raise ValueError(\"Do not recognize types={}\".format(*types))\n",
    "\n",
    "loader = DataLoader(dataset, sampler=RepeatsBatchSampler(condition_hashes))\n",
    "\n",
    "# --- compute oracles\n",
    "oracles, data = [], []\n",
    "for inputs, outputs in loader:\n",
    "    inputs = np.squeeze(inputs.cpu().numpy(), axis=0)\n",
    "    outputs = np.squeeze(outputs.cpu().numpy(), axis=0)\n",
    "    r, n = outputs.shape  # number of frame repeats, number of neurons\n",
    "    if r < 4:  # minimum number of frame repeats to be considered for oracle, free choice\n",
    "        continue\n",
    "    assert np.all(np.abs(np.diff(inputs, axis=0)) == 0), \"Images of oracle trials do not match\"\n",
    "\n",
    "    mu = outputs.mean(axis=0, keepdims=True)\n",
    "    oracle = (mu - outputs / r) * r / (r - 1)\n",
    "    oracles.append(oracle)\n",
    "    data.append(outputs)\n",
    "\n",
    "assert len(data) > 0, \"Found no oracle trials!\"\n",
    "pearson = corr(np.vstack(data), np.vstack(oracles), axis=0)\n",
    "unit_ids = dataset.neurons.unit_ids\n",
    "assert len(unit_ids) == len(pearson) == outputs.shape[-1], \"Neuron numbers do not add up\"\n",
    "print(pearson)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
